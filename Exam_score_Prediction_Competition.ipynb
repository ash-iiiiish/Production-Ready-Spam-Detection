{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ash-iiiiish/New-Project/blob/main/Exam_score_Prediction_Competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Student Scores : Competition Task"
      ],
      "metadata": {
        "id": "XQxSNkN2K1km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "DdGjPZH1coar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hbmMCihaoNDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection"
      ],
      "metadata": {
        "id": "hh_aSs1ucuet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "print(train.shape,test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V7-KMXuoPln",
        "outputId": "8a6c7a27-5e99-4639-9b0d-b8cd8100d49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(630000, 13) (270000, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fe3b4e5"
      },
      "source": [
        "## Prepare Data and Preprocessor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbbfbdc3",
        "outputId": "f9477c91-3c64-4241-a7c3-b8c8868df436"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "print(\"Columns in train DataFrame:\", train.columns.tolist())\n",
        "\n",
        "# Define target variable\n",
        "target_column_name = 'score'\n",
        "\n",
        "# Check if 'score' column exists directly\n",
        "if target_column_name not in train.columns:\n",
        "    print(f\"Warning: '{target_column_name}' column not found directly in train DataFrame. Attempting to infer target column...\")\n",
        "    # If 'score' is not found, try to infer the target column:\n",
        "    # it's typically the one present in train but not in test, and not 'id'\n",
        "    train_cols_set = set(train.columns)\n",
        "    test_cols_set = set(test.columns)\n",
        "    # Exclude 'id' as it's usually an identifier, not a feature or the target itself\n",
        "    potential_target_cols = list(train_cols_set - test_cols_set - {'id'})\n",
        "\n",
        "    if len(potential_target_cols) == 1:\n",
        "        target_column_name = potential_target_cols[0]\n",
        "        print(f\"Inferred target column name: '{target_column_name}'\")\n",
        "    elif len(potential_target_cols) > 1:\n",
        "        raise KeyError(f\"Could not uniquely identify the target column. Expected 'score' or a single unique column in train not in test. Found multiple potential target columns: {potential_target_cols}\")\n",
        "    else:\n",
        "        raise KeyError(f\"Could not identify the target column. Neither '{target_column_name}' nor a unique column from train vs. test was found.\")\n",
        "\n",
        "y = train[target_column_name]\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "# Exclude 'id' and the identified target column from features\n",
        "all_features = [col for col in train.columns if col not in ['id', target_column_name]]\n",
        "\n",
        "numerical_cols = train[all_features].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = train[all_features].select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "# 1. Define a list named feature_cols by combining numerical_cols and categorical_cols\n",
        "feature_cols = numerical_cols + categorical_cols\n",
        "\n",
        "# 2. Create a DataFrame X_full by selecting the columns specified in feature_cols from the train DataFrame.\n",
        "X_full = train[feature_cols].copy()\n",
        "\n",
        "# 3. Create a DataFrame X_test by selecting the columns specified in feature_cols from the test DataFrame.\n",
        "X_test = test[feature_cols].copy()\n",
        "\n",
        "# 4. Save the 'id' column from the original test DataFrame into a new variable, test_ids.\n",
        "test_ids = test['id']\n",
        "\n",
        "# 5. ColumnTransformer, StandardScaler, OneHotEncoder are imported above.\n",
        "\n",
        "# 6. Initialize a StandardScaler for numerical features and an OneHotEncoder with handle_unknown='ignore' for categorical features.\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# 7. Create a ColumnTransformer named preprocessor that applies the StandardScaler to the numerical_cols\n",
        "#    and the OneHotEncoder to the categorical_cols.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# 8. Fit the preprocessor on X_full and then transform both X_full and X_test\n",
        "#    using this fitted preprocessor. Store the results in X_full_processed and X_test_processed respectively.\n",
        "X_full_processed = preprocessor.fit_transform(X_full)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(\"X_full_processed shape:\", X_full_processed.shape)\n",
        "print(\"X_test_processed shape:\", X_test_processed.shape)\n",
        "print(\"test_ids head:\", test_ids.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in train DataFrame: ['id', 'age', 'gender', 'course', 'study_hours', 'class_attendance', 'internet_access', 'sleep_hours', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty', 'exam_score']\n",
            "Warning: 'score' column not found directly in train DataFrame. Attempting to infer target column...\n",
            "Inferred target column name: 'exam_score'\n",
            "X_full_processed shape: (630000, 30)\n",
            "X_test_processed shape: (270000, 30)\n",
            "test_ids head: 0    630000\n",
            "1    630001\n",
            "2    630002\n",
            "3    630003\n",
            "4    630004\n",
            "Name: id, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45e82908"
      },
      "source": [
        "## Define PyTorch MLP Regressor Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90295e7d",
        "outputId": "36e8c70f-942f-4a44-cb4d-5df2e25c1d9d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
        "        super(MLPRegressor, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        # First hidden layer\n",
        "        self.hidden_layers.append(nn.Linear(input_size, hidden_size))\n",
        "\n",
        "        # Additional hidden layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = torch.relu(layer(x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "print(\"MLPRegressor class defined successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPRegressor class defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af12087d"
      },
      "source": [
        "## Implement Optuna Objective Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e7faf09",
        "outputId": "4aa98307-082d-423e-8f24-264718785049"
      },
      "source": [
        "!pip install optuna\n",
        "import optuna\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "def objective(trial):\n",
        "    # 3. Suggest hyperparameters\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "    hidden_size = trial.suggest_int('hidden_size', 32, 256, step=32)\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5, step=0.05)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "    l2_lambda = trial.suggest_float('l2_lambda', 1e-7, 1e-5, log=True)\n",
        "\n",
        "    # 4. Split data into training and validation sets\n",
        "    # y is the target variable from previous steps\n",
        "    X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
        "        X_full_processed, y.values, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 5. Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32).reshape(-1, 1)\n",
        "    X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32)\n",
        "    y_val_tensor = torch.tensor(y_val_np, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "    # 6. Create TensorDataset and DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # 7. Determine input_size\n",
        "    input_size = X_full_processed.shape[1]\n",
        "\n",
        "    # 8. Instantiate the MLPRegressor model\n",
        "    model = MLPRegressor(\n",
        "        input_size=input_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout_rate=dropout_rate\n",
        "    )\n",
        "\n",
        "    # 9. Define criterion\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # 10. Define optimizer\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
        "\n",
        "    # 11. Training loop\n",
        "    n_epochs = 20  # Fixed number of epochs for Optuna trials\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for X_batch_val, y_batch_val in val_loader:\n",
        "                outputs_val = model(X_batch_val)\n",
        "                val_predictions.extend(outputs_val.squeeze().tolist())\n",
        "                val_targets.extend(y_batch_val.squeeze().tolist())\n",
        "\n",
        "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
        "\n",
        "        trial.report(val_rmse, epoch)\n",
        "\n",
        "        # Pruning\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    # 12. Return the final validation RMSE\n",
        "    return val_rmse\n",
        "\n",
        "print(\"Optuna objective function defined successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n",
            "Optuna objective function defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1259c93"
      },
      "source": [
        "## Run Optuna Study for Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa504db8",
        "outputId": "bf911f1d-0c58-4d55-e781-a4abd38ee542"
      },
      "source": [
        "import optuna\n",
        "\n",
        "# 2. Create an Optuna study object\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "# 3. Execute the study\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# 4. Print the best trial's value\n",
        "print(f\"Number of finished trials: {len(study.trials)}\")\n",
        "print(f\"Best trial's value (RMSE): {study.best_value}\")\n",
        "\n",
        "# 5. Print the best trial's hyperparameters\n",
        "best_params = study.best_params\n",
        "print(\"Best trial's hyperparameters:\")\n",
        "for key, value in best_params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2026-01-05 15:00:49,328] A new study created in memory with name: no-name-6ee140b7-ca97-4a3d-b65f-a473455e0355\n",
            "[I 2026-01-05 15:21:31,812] Trial 0 finished with value: 9.079089542522404 and parameters: {'learning_rate': 1.2500421558343713e-05, 'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.4, 'batch_size': 16, 'l2_lambda': 2.585251181477964e-07}. Best is trial 0 with value: 9.079089542522404.\n",
            "[I 2026-01-05 15:25:58,384] Trial 1 finished with value: 8.978055131072301 and parameters: {'learning_rate': 0.00024268397741331475, 'hidden_size': 64, 'num_layers': 3, 'dropout_rate': 0.35000000000000003, 'batch_size': 128, 'l2_lambda': 7.753951690215961e-06}. Best is trial 1 with value: 8.978055131072301.\n",
            "[I 2026-01-05 15:52:58,626] Trial 2 finished with value: 8.903771501957143 and parameters: {'learning_rate': 4.641570712658341e-05, 'hidden_size': 256, 'num_layers': 2, 'dropout_rate': 0.45, 'batch_size': 16, 'l2_lambda': 4.384373045329192e-06}. Best is trial 2 with value: 8.903771501957143.\n",
            "[I 2026-01-05 15:57:29,660] Trial 3 finished with value: 8.903995347008825 and parameters: {'learning_rate': 8.877636654181855e-05, 'hidden_size': 64, 'num_layers': 1, 'dropout_rate': 0.4, 'batch_size': 64, 'l2_lambda': 2.0718970745750472e-07}. Best is trial 2 with value: 8.903771501957143.\n",
            "[I 2026-01-05 16:29:11,753] Trial 4 finished with value: 8.888761597487058 and parameters: {'learning_rate': 4.6893569358284186e-05, 'hidden_size': 224, 'num_layers': 3, 'dropout_rate': 0.45, 'batch_size': 16, 'l2_lambda': 1.396100269301663e-06}. Best is trial 4 with value: 8.888761597487058.\n",
            "[I 2026-01-05 16:30:03,107] Trial 5 pruned. \n",
            "[I 2026-01-05 16:32:24,499] Trial 6 pruned. \n",
            "[I 2026-01-05 16:32:38,020] Trial 7 pruned. \n",
            "[I 2026-01-05 16:32:53,502] Trial 8 pruned. \n",
            "[I 2026-01-05 16:50:46,710] Trial 9 finished with value: 8.886583889014792 and parameters: {'learning_rate': 6.347589173153159e-05, 'hidden_size': 32, 'num_layers': 3, 'dropout_rate': 0.0, 'batch_size': 16, 'l2_lambda': 1.1816257860374886e-06}. Best is trial 9 with value: 8.886583889014792.\n",
            "[I 2026-01-05 16:57:54,059] Trial 10 finished with value: 8.879828942768595 and parameters: {'learning_rate': 0.0009785892523556234, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.0, 'batch_size': 32, 'l2_lambda': 5.181165367020812e-07}. Best is trial 10 with value: 8.879828942768595.\n",
            "[I 2026-01-05 17:05:07,505] Trial 11 finished with value: 8.887038569020499 and parameters: {'learning_rate': 0.0006794841301433881, 'hidden_size': 128, 'num_layers': 1, 'dropout_rate': 0.0, 'batch_size': 32, 'l2_lambda': 5.029661832385962e-07}. Best is trial 10 with value: 8.879828942768595.\n",
            "[I 2026-01-05 17:14:39,552] Trial 12 finished with value: 8.872869001490486 and parameters: {'learning_rate': 3.0064750039788855e-05, 'hidden_size': 128, 'num_layers': 2, 'dropout_rate': 0.0, 'batch_size': 32, 'l2_lambda': 5.353057656074515e-07}. Best is trial 12 with value: 8.872869001490486.\n",
            "[I 2026-01-05 17:15:02,632] Trial 13 pruned. \n"
          ]
        }
      ]
    }
  ]
}